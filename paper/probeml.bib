@inproceedings{GPyTorch,
 author = {Gardner, Jacob and Pleiss, Geoff and Weinberger, Kilian Q and Bindel, David and Wilson, Andrew G},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {GPyTorch: Blackbox Matrix-Matrix Gaussian Process Inference with GPU Acceleration},
 url = {https://proceedings.neurips.cc/paper/2018/file/27e8e17134dd7083b050476733207ea1-Paper.pdf},
 volume = {31},
 year = {2018}
}

@ARTICLE{frenkel,
AUTHOR={Frenkel, Charlotte and Lefebvre, Martin and Bol, David},   
TITLE={Learning Without Feedback: Fixed Random Learning Signals Allow for Feedforward Training of Deep Neural Networks},      
JOURNAL={Frontiers in Neuroscience},      
VOLUME={15},
PAGES={20},
YEAR={2021},
URL={https://www.frontiersin.org/article/10.3389/fnins.2021.629892},       
DOI={10.3389/fnins.2021.629892},      
ISSN={1662-453X},   
ABSTRACT={While the backpropagation of error algorithm enables deep neural network training, it implies (i) bidirectional synaptic weight transport and (ii) update locking until the forward and backward passes are completed. Not only do these constraints preclude biological plausibility, but they also hinder the development of low-cost adaptive smart sensors at the edge, as they severely constrain memory accesses and entail buffering overhead. In this work, we show that the one-hot-encoded labels provided in supervised classification problems, denoted as targets, can be viewed as a proxy for the error sign. Therefore, their fixed random projections enable a layerwise feedforward training of the hidden layers, thus solving the weight transport and update locking problems while relaxing the computational and memory requirements. Based on these observations, we propose the direct random target projection (DRTP) algorithm and demonstrate that it provides a tradeoff between accuracy and computational cost that is suitable for adaptive edge computing devices.}
}

@ARTICLE{Pinckaers,
  author={Pinckaers, Hans and van Ginneken, Bram and Litjens, Geert},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={Streaming convolutional neural networks for end-to-end learning with multi-megapixel images}, 
  year={2020},
  volume={},
  number={},
  pages={1-1},
  doi={10.1109/TPAMI.2020.3019563}
}

@article{TroppLR,
author = {Tropp, Joel A. and Yurtsever, Alp and Udell, Madeleine and Cevher, Volkan},
title = {Streaming Low-Rank Matrix Approximation with an Application to Scientific Simulation},
journal = {SIAM Journal on Scientific Computing},
volume = {41},
number = {4},
pages = {A2430-A2463},
year = {2019},
doi = {10.1137/18M1201068},
URL = {https://doi.org/10.1137/18M1201068},
eprint = {https://doi.org/10.1137/18M1201068}}

@INPROCEEDINGS{wangacccnn,
  author={Wang, Ziheng and Nelaturu, Sree Harsha and Amarasinghe, Saman},
  booktitle={2019 2nd Workshop on Energy Efficient Machine Learning and Cognitive Computing for Embedded Applications (EMC2)}, 
  title={Accelerated CNN Training through Gradient Approximation}, 
  year={2019},
  volume={},
  number={},
  pages={31-35},
  doi={10.1109/EMC249363.2019.00014}}

@conference{LARS,
title = "On large-batch training for deep learning: Generalization gap and sharp minima",
abstract = "The stochastic gradient descent (SGD) method and its variants are algorithms of choice for many Deep Learning tasks. These methods operate in a small-batch regime wherein a fraction of the training data, say 32-512 data points, is sampled to compute an approximation to the gradient. It has been observed in practice that when using a larger batch there is a degradation in the quality of the model, as measured by its ability to generalize. We investigate the cause for this generalization drop in the large-batch regime and present numerical evidence that supports the view that large-batch methods tend to converge to sharp minimizers of the training and testing functions-and as is well known, sharp minima lead to poorer generalization. In contrast, small-batch methods consistently converge to flat minimizers, and our experiments support a commonly held view that this is due to the inherent noise in the gradient estimation. We discuss several strategies to attempt to help large-batch methods eliminate this generalization gap.",
author = "Keskar, {Nitish Shirish} and Jorge Nocedal and Tang, {Ping Tak Peter} and Dheevatsa Mudigere and Mikhail Smelyanskiy",
year = "2017",
language = "English (US)",
note = "5th International Conference on Learning Representations, ICLR 2017 ; Conference date: 24-04-2017 Through 26-04-2017",
}

@InProceedings{nokland19a,
  title = 	 {Training Neural Networks with Local Error Signals},
  author =       {N{\o}kland, Arild and Eidnes, Lars Hiller},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  pages = 	 {4839--4850},
  year = 	 {2019},
  editor = 	 {Kamalika Chaudhuri and Ruslan Salakhutdinov},
  volume = 	 {97},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {09--15 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v97/nokland19a/nokland19a.pdf},
  url = 	 {
http://proceedings.mlr.press/v97/nokland19a.html
},
  abstract = 	 {Supervised training of neural networks for classification is typically performed with a global loss function. The loss function provides a gradient for the output layer, and this gradient is back-propagated to hidden layers to dictate an update direction for the weights. An alternative approach is to train the network with layer-wise loss functions. In this paper we demonstrate, for the first time, that layer-wise training can approach the state-of-the-art on a variety of image datasets. We use single-layer sub-networks and two different supervised loss functions to generate local error signals for the hidden layers, and we show that the combination of these losses help with optimization in the context of local learning. Using local errors could be a step towards more biologically plausible deep learning because the global error does not have to be transported back to hidden layers. A completely backprop free variant outperforms previously reported results among methods aiming for higher biological plausibility.}
}

@inproceedings{Kaperick2019DiagonalEW,
  title={Diagonal Estimation with Probing Methods},
  author={Bryan Kaperick},
  year={2019}
}

@misc{cortinovis2020randomized,
      title={On randomized trace estimates for indefinite matrices with an application to determinants}, 
      author={Alice Cortinovis and Daniel Kressner},
      year={2020},
      eprint={2005.10009},
      archivePrefix={arXiv},
      primaryClass={math.NA}
}

@ARTICLE{hutchpp,
       author = {{Meyer}, Raphael A. and {Musco}, Cameron and {Musco}, Christopher and {Woodruff}, David P.},
        title = "{Hutch++: Optimal Stochastic Trace Estimation}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Data Structures and Algorithms, Computer Science - Machine Learning, Mathematics - Numerical Analysis},
         year = 2020,
        month = oct,
          eid = {arXiv:2010.09649},
        pages = {arXiv:2010.09649},
archivePrefix = {arXiv},
       eprint = {2010.09649},
 primaryClass = {cs.DS},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2020arXiv201009649M},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@article{Avron,
author = {Avron, Haim and Toledo, Sivan},
title = {Randomized Algorithms for Estimating the Trace of an Implicit Symmetric Positive Semi-Definite Matrix},
year = {2011},
issue_date = {April 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {58},
number = {2},
issn = {0004-5411},
url = {https://doi.org/10.1145/1944345.1944349},
doi = {10.1145/1944345.1944349},
abstract = {We analyze the convergence of randomized trace estimators. Starting at 1989, several algorithms have been proposed for estimating the trace of a matrix by 1/MΣi=1M ziT Azi, where the zi are random vectors; different estimators use different distributions for the zis, all of which lead to E(1/MΣi=1M ziT Azi) = trace(A). These algorithms are useful in applications in which there is no explicit representation of A but rather an efficient method compute zTAz given z. Existing results only analyze the variance of the different estimators. In contrast, we analyze the number of samples M required to guarantee that with probability at least 1-δ, the relative error in the estimate is at most ϵ. We argue that such bounds are much more useful in applications than the variance. We found that these bounds rank the estimators differently than the variance; this suggests that minimum-variance estimators may not be the best.We also make two additional contributions to this area. The first is a specialized bound for projection matrices, whose trace (rank) needs to be computed in electronic structure calculations. The second is a new estimator that uses less randomness than all the existing estimators.},
journal = {J. ACM},
month = apr,
articleno = {8},
numpages = {34},
keywords = {Trace estimation, implicit linear operators}
}

@article{2014cudnn,
   author={Chetlur, S. and Woolley, C. and Vandermersch, P. and Cohen, J. and Tran, J. and Catanzaro, B. and Shelhamer, E.},
   title={"{cuDNN: Efficient Primitives for Deep Learning}"},
   journal={ArXiv e-prints},
   archivePrefix={"arXiv"},
   eprint={1410.0759},
   keywords={Computer Science – Neural and Evolutionary Computing, Computer Science – Learning, Computer Science – Mathematical Software},
   year={2014},
   month={oct},
   adsurl={http://adsabs.harvard.edu/abs/2014arXiv1410.0759C},
   adsnote={Provided by the SAO/NASA Astrophysics Data System}
}

@inproceedings{DFA1,
author = {N\o{}kland, Arild},
title = {Direct Feedback Alignment Provides Learning in Deep Neural Networks},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Artificial neural networks are most commonly trained with the back-propagation algorithm, where the gradient for learning is provided by back-propagating the error, layer by layer, from the output layer to the hidden layers. A recently discovered method called feedback-alignment shows that the weights used for propagating the error backward don't have to be symmetric with the weights used for propagation the activation forward. In fact, random feedback weights work evenly well, because the network learns how to make the feedback useful. In this work, the feedback alignment principle is used for training hidden layers more independently from the rest of the network, and from a zero initial condition. The error is propagated through fixed random feedback connections directly from the output layer to each hidden layer. This simple method is able to achieve zero training error even in convolutional networks and very deep networks, completely without error back-propagation. The method is a step towards biologically plausible machine learning because the error signal is almost local, and no symmetric or reciprocal weights are required. Experiments show that the test performance on MNIST and CIFAR is almost as good as those obtained with back-propagation for fully connected networks. If combined with dropout, the method achieves 1.45% error on the permutation invariant MNIST task.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {1045–1053},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@misc{oktay2020randomized,
    title={Randomized Automatic Differentiation},
    author={Deniz Oktay and Nick McGreivy and Joshua Aduol and Alex Beatson and Ryan P. Adams},
    year={2020},
    eprint={2007.10412},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@article{DBLP,
  author    = {Priya Goyal and
               Piotr Doll{\'{a}}r and
               Ross B. Girshick and
               Pieter Noordhuis and
               Lukasz Wesolowski and
               Aapo Kyrola and
               Andrew Tulloch and
               Yangqing Jia and
               Kaiming He},
  title     = {Accurate, Large Minibatch {SGD:} Training ImageNet in 1 Hour},
  journal   = {CoRR},
  volume    = {abs/1706.02677},
  year      = {2017},
  url       = {http://arxiv.org/abs/1706.02677},
  archivePrefix = {arXiv},
  eprint    = {1706.02677},
  timestamp = {Mon, 13 Aug 2018 16:49:10 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/GoyalDGNWKTJH17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@incollection{NEURIPS2019_9015,
title = {PyTorch: An Imperative Style, High-Performance Deep Learning Library},
author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
booktitle = {Advances in Neural Information Processing Systems 32},
editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
pages = {8024--8035},
year = {2019},
publisher = {Curran Associates, Inc.},
url = {http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf}
}

@article{Flux.jl-2018,
  author    = {Michael Innes and
               Elliot Saba and
               Keno Fischer and
               Dhairya Gandhi and
               Marco Concetto Rudilosso and
               Neethu Mariya Joy and
               Tejan Karmali and
               Avik Pal and
               Viral Shah},
  title     = {Fashionable Modelling with Flux},
  journal   = {CoRR},
  volume    = {abs/1811.01457},
  year      = {2018},
  url       = {https://arxiv.org/abs/1811.01457},
  archivePrefix = {arXiv},
  eprint    = {1811.01457},
  timestamp = {Thu, 22 Nov 2018 17:58:30 +0100},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1811-01457}}

  @INPROCEEDINGS{lightonproj,
  author={Saade, A. and Caltagirone, F. and Carron, I. and Daudet, L. and Drémeau, A. and Gigan, S. and Krzakala, F.},
  booktitle={2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 
  title={Random projections through multiple optical scattering: Approximating Kernels at the speed of light}, 
  year={2016},
  volume={},
  number={},
  pages={6215-6219},
  doi={10.1109/ICASSP.2016.7472872}}